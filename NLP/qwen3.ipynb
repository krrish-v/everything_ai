{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972919b7-890c-416b-ba3e-42b10d7cd4b3",
   "metadata": {},
   "source": [
    "## AMD GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197e128b-3ed2-44cd-b30f-48b5b7287fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e412e38f-bb37-4cae-9d99-fe656c7618c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSA_OVERRIDE_GFX_VERSION is set to: 11.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '11.0.0'\n",
    "\n",
    "gfx_version = os.getenv('HSA_OVERRIDE_GFX_VERSION')\n",
    "print(f\"HSA_OVERRIDE_GFX_VERSION is set to: {gfx_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6a803b-0892-4ced-845c-a30971032171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9486b8a-1a00-42e9-8c17-adefea23fc4e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e414bd05-5764-41bf-aef9-92ebae95ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e92bd3-9c3c-4117-9636-8640c1c85ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ").to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1070b032-e542-47cd-9ff4-cd85a357ed4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0769e62-57f9-42de-b61b-75f2a0c69460",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efee9ae9-8483-4bf3-9884-eabe71b25805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ffbf03-6f2e-494b-9bc8-136fbe07d99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  35127,    752,    264,   2805,  16800,    311,\n",
       "           3460,   4128,   1614,     13, 151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e99d4b1-7e84-49ed-baa9-ca4500220d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b40e85-ae0b-41c0-a097-03c4cf3d8a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5180be6-72cf-47ba-b060-17b3591625ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f36049-71e2-4cb7-a9a5-4e14680f7b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40290e89-7ba9-4344-82e1-1c488ec196f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53cd2c3-1858-4cbc-a216-8426664b973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user wants a short introduction to a large language model. Let me start by recalling what I know about LLMs. They\\'re big, powerful models used for tasks like writing, translation, etc. I should mention their capabilities.\\n\\nI need to keep it concise. Maybe start with the basics: \"Large Language Models (LLMs) are AI systems designed to understand and generate human language.\" Then talk about their main functions, like text generation, translation, and answering questions. \\n\\nWait, should I include something about training data? Probably yes. LLMs are trained on vast amounts of text to understand context. Also, mention their applications: creative writing, customer service, etc. \\n\\nMake sure the tone is friendly and informative. Avoid jargon. Keep it simple. Let me check if I\\'m missing anything. Oh, maybe mention that they can be customized for specific tasks. Alright, that should cover it.\\n</think>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinking_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a458b5c1-8531-499a-9fe0-ac7d220c8dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) are AI systems designed to understand and generate human language. They are trained on vast amounts of text to grasp context and learn patterns, enabling tasks like writing, translation, and answering questions. These models can be customized for specific applications, making them versatile tools for creative and professional tasks.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e7b06-3c82-4054-8170-fe34e04db491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
